{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "927a8b8b-b14f-4b9a-8ff1-65b9ab7be841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import requests\n",
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "\n",
    "\n",
    "@client_tool\n",
    "def knowledge_search(query: str):\n",
    "    \"\"\"Search for information in a database. Do not use this tool to load files.\n",
    "\n",
    "    :param query: The query to search for. Can be a natural language sentence or keywords.\n",
    "    \"\"\"\n",
    "    if \"nba\" not in query.lower():\n",
    "        override_output = f\"Found context for query '{query}': The open-source AI models you can fine-tune, distill and deploy anywhere. Choose from our collection of models: Llama 3.1, Llama 3.2, Llama 3.3.\"\n",
    "    else:\n",
    "        override_output = f\"Found context for query '{query}': The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\"\n",
    "    return {\n",
    "        \"content\": override_output\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "feb2e74e-e94c-4042-87b1-07d173a42984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User> Write code to load this csv file, use the `code_interpreter` tool to execute it, then tell me what's in it\n",
      "\n",
      "inference> \n",
      "\n",
      "import pandas as pd\n",
      "# Load data\n",
      "df = pd\n",
      ".read_csv('/var/folders/cz/vyh7y1d11xg881lsx\n",
      "sshnc5c0000gn/T/tmpom3cghqv/b3H9u7\n",
      "Scinflation.csv')\n",
      "# Rows\n",
      "print(\"Number of rows and columns in the data:\", df\n",
      ".shape)\n",
      "# Columns\n",
      "print(\"Columns of the data are:\", len(df.columns))\n",
      "# Column names\n",
      "\n",
      "print(\"Columns of the data are:\", df.columns)\n",
      "# Column dtypes\n",
      "print(\"Dat\n",
      "atype of the columns are:\", df.dtypes)\n",
      "# Sample of data\n",
      "print\n",
      "(\"Data sample from file:\")\n",
      "print(df.head())\n",
      "\n",
      "tool_execution> Tool:code_interpreter Args:{'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\\'/var/folders/cz/vyh7y1d11xg881lsxsshnc5c0000gn/T/tmpom3cghqv/b3H9u7Scinflation.csv\\')\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)\\n# Sample of data\\nprint(\"Data sample from file:\")\\nprint(df.head())'}\n",
      "tool_execution> Tool:code_interpreter Response:\n",
      "        Year,Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec\n",
      "2014,1.6,1.6,1.7,1.8,2.0,1.9,1.9,1.7,1.7,1.8,1.7,1.6\n",
      "2015,1.6,1.7,1.8,1.8,1.7,1.8,1.8,1.8,1.9,1.9,2.0,2.1\n",
      "2016,2.2,2.3,2.2,2.1,2.2,2.2,2.2,2.3,2.2,2.1,2.1,2.2\n",
      "2017,2.3,2.2,2.0,1.9,1.7,1.7,1.7,1.7,1.7,1.8,1.7,1.8\n",
      "2018,1.8,1.8,2.1,2.1,2.2,2.3,2.4,2.2,2.2,2.1,2.2,2.2\n",
      "2019,2.2,2.1,2.0,2.1,2.0,2.1,2.2,2.4,2.4,2.3,2.3,2.3\n",
      "2020,2.3,2.4,2.1,1.4,1.2,1.2,1.6,1.7,1.7,1.6,1.6,1.6\n",
      "2021,1.4,1.3,1.6,3.0,3.8,4.5,4.3,4.0,4.0,4.6,4.9,5.5\n",
      "2022,6.0,6.4,6.5,6.2,6.0,5.9,5.9,6.3,6.6,6.3,6.0,5.7\n",
      "2023,5.6,5.5,5.6,5.5,5.3,4.8,4.7,4.3,4.1,4.0,4.0,3.9\n",
      "\n",
      "inference> \n",
      "The\n",
      " CSV file contains inflation data for the years \n",
      "2014 to 2023. The data includes the year and\n",
      " the inflation rate for each month of the year. The inflation\n",
      " rate is represented as a percentage. The data is organized in a\n",
      " table format with the year in the first column and the inflation rates for\n",
      " each month in the subsequent columns.\n",
      "\n",
      "\n",
      "User> when was the perplexity created?\n",
      "\n",
      "inference> \n",
      "[k\n",
      "nowledge_search(query=\"perplexity creation date\")]\n",
      "\n",
      "tool_execution> Tool:knowledge_search Args:{'query': 'perplexity creation date', 'vector_db_ids': ['test-vector-db-19d5e8e6-96ce-4814-a8c1-6a01038fb978']}\n",
      "tool_execution> Tool:knowledge_search Response:[TextContentItem(text='id:perplexity_wiki; content:Perplexity was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\nSrinivas, the CEO, worked at OpenAI as an AI researcher.\\nKonwinski was among the founding team at Databricks.\\nYarats, the CTO, was an AI research scientist at Meta.\\nHo, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]', type='text'), TextContentItem(text='id:nba_wiki; content:The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).', type='text'), TextContentItem(text='id:perplexity_wiki; content:, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]', type='text')]\n",
      "inference> \n",
      "The\n",
      " perplexity was created in 2022.\n",
      "\n",
      "\n",
      "User> when was the nba created?\n",
      "\n",
      "inference> \n",
      "[k\n",
      "nowledge_search(query=\"NBA creation date\")]\n",
      "\n",
      "tool_execution> Tool:knowledge_search Args:{'query': 'NBA creation date', 'vector_db_ids': ['test-vector-db-19d5e8e6-96ce-4814-a8c1-6a01038fb978']}\n",
      "tool_execution> Tool:knowledge_search Response:[TextContentItem(text='id:nba_wiki; content:The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).', type='text'), TextContentItem(text='id:perplexity_wiki; content:Perplexity was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\nSrinivas, the CEO, worked at OpenAI as an AI researcher.\\nKonwinski was among the founding team at Databricks.\\nYarats, the CTO, was an AI research scientist at Meta.\\nHo, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]', type='text'), TextContentItem(text='id:perplexity_wiki; content:, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]', type='text')]\n",
      "inference> \n",
      "The\n",
      " NBA was created on August 3, 1949, with the merger\n",
      " of the Basketball Association of America (BAA) and the National Basketball League\n",
      " (NBL).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types import Document\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "from termcolor import cprint\n",
    "from uuid import uuid4\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "llama_stack_client = LlamaStackClient(\n",
    "    base_url=f\"http://localhost:{os.environ['LLAMA_STACK_PORT']}\"\n",
    ")\n",
    "\n",
    "urls = [\"chat.rst\"]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "documents = []\n",
    "documents.append(\n",
    "    Document(\n",
    "        document_id=\"nba_wiki\",\n",
    "        content=\"The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\",\n",
    "        metadata={},\n",
    "    )\n",
    ")\n",
    "documents.append(\n",
    "    Document(\n",
    "        document_id=\"perplexity_wiki\",\n",
    "        content=\"\"\"Perplexity was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\n",
    "\n",
    "Srinivas, the CEO, worked at OpenAI as an AI researcher.\n",
    "Konwinski was among the founding team at Databricks.\n",
    "Yarats, the CTO, was an AI research scientist at Meta.\n",
    "Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\"\"\",\n",
    "        metadata={},\n",
    "    )\n",
    ")\n",
    "vector_db_id = f\"test-vector-db-{uuid4()}\"\n",
    "llama_stack_client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")\n",
    "llama_stack_client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=128,\n",
    ")\n",
    "\n",
    "client_tools = (knowledge_search,)\n",
    "client_tools = []\n",
    "\n",
    "# with system message behavior replace\n",
    "instructions = \"\"\"\n",
    "You are a helpful assistant. You have access to functions, but you should only use them if they are required.\n",
    "\n",
    "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
    "Based on the question, you may or may not need to make one function/tool call to achieve the purpose.\n",
    "If none of the function can be used, don't return [], instead answer the question directly without using functions. If the given question lacks the parameters required by the function,\n",
    "also point it out.\n",
    "Do not make the same function call twice as you will get the same result.\n",
    "Do not make another function call consecutively. Answer the user query after getting the result from the function call.\n",
    "Only use `code_interpreter` to answer questions about the provided file.\n",
    "\n",
    "{{ function_description }}\n",
    "\"\"\"\n",
    "# instructions = \"You are a helpful assistant.\"\n",
    "agent_config = {\n",
    "    \"toolgroups\": [\n",
    "        dict(\n",
    "            name=\"builtin::rag\",\n",
    "            args={\"vector_db_ids\": [vector_db_id]},\n",
    "        ),\n",
    "        \"builtin::code_interpreter\",\n",
    "    ],\n",
    "    \"instructions\": instructions,\n",
    "    # \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"model\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"tool_config\": {\n",
    "        \"system_message_behavior\": \"replace\",\n",
    "    },\n",
    "    \"client_tools\":[client_tool.get_tool_definition() for client_tool in client_tools],\n",
    "\n",
    "    \"enable_session_persistence\": False,\n",
    "}\n",
    "\n",
    "agent = Agent(llama_stack_client, agent_config, client_tools)\n",
    "inflation_doc = Document(\n",
    "    document_id=\"test_csv\",\n",
    "    content=\"https://raw.githubusercontent.com/meta-llama/llama-stack-apps/main/examples/resources/inflation.csv\",\n",
    "    mime_type=\"text/csv\",\n",
    "    metadata={},\n",
    ")\n",
    "user_prompts = [\n",
    "    (\n",
    "        \"Write code to load this csv file, use the `code_interpreter` tool to execute it, then tell me what's in it\",\n",
    "        [inflation_doc],\n",
    "        \"code_interpreter\",\n",
    "    ),\n",
    "    # (\n",
    "    #     \"Use the code_interpreter tool to compute 20+12, then answer with the output\",\n",
    "    #     [],\n",
    "    #     \"code_interpreter\",\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"Use 'knowledge_search' function to answer the question: what are the versions of Llama3? do not use code_interpreter\",\n",
    "    #     [],\n",
    "    #     \"knowledge_search\",\n",
    "    # ),\n",
    "    (\n",
    "        \"when was the perplexity created?\",\n",
    "        [],\n",
    "        \"knowledge_search\",\n",
    "    ),\n",
    "    (\n",
    "        \"when was the nba created?\",\n",
    "        [],\n",
    "        \"knowledge_search\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for prompt, docs, tool_name in user_prompts:\n",
    "    print(f\"User> {prompt}\")\n",
    "    session_id = agent.create_session(f\"test-session-{uuid4()}\")\n",
    "    response = agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        documents=docs,\n",
    "    )\n",
    "    logs = [str(log) for log in EventLogger().log(response) if log is not None]\n",
    "    logs_str = \"\\n\".join(logs)\n",
    "    print(logs_str)\n",
    "    assert f\"Tool:{tool_name}\" in logs_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e6ce012-b566-40f3-b0c2-fabf8f593d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import requests\n",
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "\n",
    "\n",
    "@client_tool\n",
    "def load_url(url: str):\n",
    "    \"\"\"Load the content given a URL\n",
    "\n",
    "    :param url: The url to load\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"content\": \"\"\"\n",
    "Today Google announced that they have released the source code to PebbleOS. This is massive for Rebble, and will accelerate our efforts to produce new hardware.\n",
    "\n",
    "Previously, we have been working on our own replacement firmware: RebbleOS. As you can see by the commit history though, progress was slow. Building a production-ready realtime OS for the Pebble is no small feat, and although we were confident we’d get there given enough time, it was never our ideal path. Thanks to the hard work of many people both within Google and not, we finally have our hands on the original source code for PebbleOS. You can read Google’s blog post on this for even more information.\n",
    "\n",
    "This does not mean we instantly have the ability to start developing updates for PebbleOS though, we first will need to spend some concentrated time getting it to build. But before we talk about that, let’s talk about Rebble itself.\n",
    "\"\"\"\n",
    "    }\n",
    "    # get the url's contenet\n",
    "    response = requests.get(url)\n",
    "    return {\"content\": response.text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32e82f13-67f2-4b3e-8c62-cc846163cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'load_url', 'description': 'Load the content given a URL', 'parameters': [{'name': 'url', 'description': 'The url to load', 'parameter_type': 'str', 'default': None, 'required': True}], 'metadata': {}, 'tool_prompt_format': 'python_list'}]\n",
      "\u001b[32mUser> tell me a joke\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mWhy\u001b[0m\u001b[33m don't skeletons fight each other?\n",
      "\n",
      "\u001b[97m\u001b[0mhey don't have the guts!\u001b[0m\n",
      "\u001b[30m\u001b[0m\u001b[32mUser> load https://llama-stack.readthedocs.io/en/latest/introduction/index.html and summarize it\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33mload_url(url=\"https://llama-stack.readthedocs.io/en/latest\u001b[0m\u001b[33m/introduction/index.html\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mCustomTool> {\"content\": \"\\nToday Google announced that they have released the source code to PebbleOS. This is massive for Rebble, and will accelerate our efforts to produce new hardware.\\n\\nPreviously, we have been working on our own replacement firmware: RebbleOS. As you can see by the commit history though, progress was slow. Building a production-ready realtime OS for the Pebble is no small feat, and although we were confident we’d get there given enough time, it was never our ideal path. Thanks to the hard work of many people both within Google and not, we finally have our hands on the original source code for PebbleOS. You can read Google’s blog post on this for even more information.\\n\\nThis does not mean we instantly have the ability to start developing updates for PebbleOS though, we first will need to spend some concentrated time getting it to build. But before we talk about that, let’s talk about Rebble itself.\\n\"}\u001b[0m\n",
      "\u001b[30m\u001b[0m\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m article from the given URL discusses Google's release\u001b[0m\u001b[33m of the source code for PebbleOS,\u001b[0m\u001b[33m a real-time operating system used in Pebble\u001b[0m\u001b[33m smartwatches. The author of the article\u001b[0m\u001b[33m mentions that they had been working on their own\u001b[0m\u001b[33m replacement firmware, RebbleOS, but decided\u001b[0m\u001b[33m to use the open-source PebbleOS instead\u001b[0m\u001b[33m. The article concludes by stating that the author\u001b[0m\u001b[33m's team will need to spend time getting\u001b[0m\u001b[33m PebbleOS to build, but they are\u001b[0m\u001b[33m excited to start working on updates for the operating\u001b[0m\u001b[33m system.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types import Document\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "\n",
    "\n",
    "\n",
    "def create_http_client():\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "\n",
    "    return LlamaStackClient(\n",
    "        base_url=f\"http://localhost:{os.environ['LLAMA_STACK_PORT']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_library_client(template=\"ollama\"):\n",
    "    from llama_stack import LlamaStackAsLibraryClient\n",
    "\n",
    "    client = LlamaStackAsLibraryClient(template)\n",
    "    client.initialize()\n",
    "    return client\n",
    "\n",
    "\n",
    "# client = create_library_client()  # or create_http_client() depending on the environment you picked\n",
    "\n",
    "\n",
    "client = (\n",
    "    create_http_client()\n",
    ")  # or create_http_client() depending on the environment you picked\n",
    "\n",
    "# Documents to be used for RAG\n",
    "urls = [\"chat.rst\", \"llama3.rst\", \"datasets.rst\", \"lora_finetune.rst\"]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "documents.append(\n",
    "    Document(\n",
    "        document_id=f\"num-{10}\",\n",
    "        content=\"\"\"Llama Stack defines and standardizes the core building blocks needed to bring generative AI applications to market. It provides a unified set of APIs with implementations from leading service providers, enabling seamless transitions between development and production environments. More specifically, it provides\n",
    "\n",
    "Unified API layer for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.\n",
    "\n",
    "Plugin architecture to support the rich ecosystem of implementations of the different APIs in different environments like local development, on-premises, cloud, and mobile.\n",
    "\n",
    "Prepackaged verified distributions which offer a one-stop solution for developers to get started quickly and reliably in any environment\n",
    "\n",
    "Multiple developer interfaces like CLI and SDKs for Python, Node, iOS, and Android\n",
    "\n",
    "Standalone applications as examples for how to build production-grade AI applications with Llama Stack\"\"\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    ")\n",
    "\n",
    "# Register a vector database\n",
    "vector_db_id = f\"test-vector-db-{uuid.uuid4().hex}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")\n",
    "\n",
    "# Insert the documents into the vector database\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")\n",
    "\n",
    "client_tools = (load_url,)\n",
    "\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a helpful assistant. You have access to functions, but you should only use them if they are required.\n",
    "\n",
    "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
    "Based on the question, you may or may not need to make one or more function/tool calls to achieve the purpose.\n",
    "If none of the function can be used, don't return [], instead answer the question directly without using functions. If the given question lacks the parameters required by the function,\n",
    "also point it out. \n",
    "\n",
    "{{ function_description }}\n",
    "\"\"\"\n",
    "\n",
    "# instructions = \"You are a helpful assistant.\"\n",
    "\n",
    "# instructions = \"\"\"\n",
    "# You are an expert in composing functions. You are given a question and a set of possible functions.\n",
    "# Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
    "# If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
    "# also point it out.\n",
    "# \"\"\"\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    model=os.environ[\"INFERENCE_MODEL\"],\n",
    "    # Define instructions for the agent ( aka system prompt)\n",
    "    instructions=instructions,\n",
    "    enable_session_persistence=False,\n",
    "    # Define tools available to the agent\n",
    "    toolgroups=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag\",\n",
    "            \"args\": {\n",
    "                \"vector_db_ids\": [vector_db_id],\n",
    "            },\n",
    "        },\n",
    "        \"builtin::code_interpreter\",\n",
    "    ],\n",
    "    client_tools=[client_tool.get_tool_definition() for client_tool in client_tools],\n",
    "    tool_config={\n",
    "        \"tool_choice\": \"auto\",\n",
    "        \"tool_prompt_format\": \"python_list\",\n",
    "        \"system_message_behavior\": \"replace\",\n",
    "    },\n",
    "    max_infer_iters=10,\n",
    ")\n",
    "print([client_tool.get_tool_definition() for client_tool in client_tools])\n",
    "\n",
    "rag_agent = Agent(client, agent_config, client_tools)\n",
    "session_id = rag_agent.create_session(\"test-session\")\n",
    "\n",
    "user_prompts = [\n",
    "    # \"What are the top 5 topics that were explained? Only list succinct bullet points.\",\n",
    "    # \"search in the vector database for the term 'torchtune'\",\n",
    "    # \"use the search tool and answer what is llama-stack?\",\n",
    "    # \"can you summarize https://llama-stack.readthedocs.io/en/latest/introduction/index.html\",\n",
    "    \"tell me a joke\",\n",
    "    \"load https://llama-stack.readthedocs.io/en/latest/introduction/index.html and summarize it\",\n",
    "]\n",
    "\n",
    "# Run the agent loop by calling the `create_turn` method\n",
    "for prompt in user_prompts:\n",
    "    cprint(f\"User> {prompt}\", \"green\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()\n",
    "\n",
    "\n",
    "# response = rag_agent.create_turn(\n",
    "#     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#     session_id=session_id,\n",
    "#     stream=False,\n",
    "# )\n",
    "# print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
